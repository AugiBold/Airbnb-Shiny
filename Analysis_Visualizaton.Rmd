---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
```
Introduction: 
Airbnb, more than just a vacation rental company, was one of the market disruptors that started in 2008 which destabilized the hotel industry and changed the way people traveled by offering more options in booking places to stay.  It got so popular that more and more people started using it as a business which eventually led to certain cities to put in place regulations on short term rentals.  
Data: 
One of those cities is Washington, D.C and I obtained my data from insideairbnb.com, an independent non-commercial project that gathers publicly available Airbnb information with the express purpose of promoting awareness on this issue of Airbnb adversely affecting local neighborhoods and established real-estate businesses.  The dataset only provided listings that were available for the last 12 months in Washington, D.C.  
Objective:
With that in mind, through this project I wanted to explore 2 questions particularly in the current post Covid state:
1) As a potential guest, where are the more affordable yet well rated and reviewed listings? 
2) As a potential host, which area has the most demand and likelihood of occupancy in the D.C market for listings? 

The main data I will be working with is the listings data and it includes all kinds of data from which I selected data regarding the following:listings, coordinate location, neighborhood, price per night, room type, number of reviews, review scores, and availability days per year.

Read in the data: 
```{r}
listings = read.csv('./listings.csv')
listings_detailed = read.csv('./listings_detailed.csv')
head(listings)
head(listings_detailed)

```
I selected fields pertaining to my question of interest from the main dataset and also extracted the review-score data from the listings_detailed dataset and joined it to the main dataset to capture the rating information.  
```{r}
listings = listings %>% select('id', 'name', 'host_id', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'number_of_reviews', 'calculated_host_listings_count', 'availability_365') 
head(listings, 3)
```   
Join the data and clean up the long names into short, to-the-point column names.    
```{r}
rating = listings_detailed %>% select('id', 'review_scores_rating')
listings = inner_join(listings, rating, by = 'id')
listings = listings %>% rename(host_listings = 'calculated_host_listings_count', availability = 'availability_365', rating = 'review_scores_rating')
head(listings)
```

Let's get a general sense of the data and look for null values: 
```{r}
str(listings)
summary(listings)
colSums(is.na(listings))
```
As we can see, the rating field has 2373 NULL/NaN values making up about 30% of all the data.  
For general analysis, we will keep the data for sure, but when we are analyzing rating feature alone, we will only consider the non-missing data.    
This sort of correlates with the number of reviews having 0 values when checked: 

```{r}
listings %>% filter(number_of_reviews==0) %>% summarise(count = n())
```
Noticed that there were 6 listings with $0 price which doesn't make sense so I excluded them: 
```{r}
listings = listings %>% filter(price > 0)
dim(listings)
```

Let's look at the General Distribution of the Prices to get a feel for the pricing landscape:
Because 94% of listings fall under $500 per night, I plotted the distribution within this range for better visibility. 

```{r}
listings %>% filter(price < 500) %>% summarise(count = n(), under_500_pct = count/length(listings$price)*100) 
```


```{r}
price_distribution = listings %>% filter(price < 500) %>% ggplot(aes(x = price)) +
  geom_histogram(aes(y = ..density..), color = "blue", fill = "darkseagreen3") + geom_density(alpha = .2, fill="chartreuse") + xlab('Price ($)') + ylab('Density') + ggtitle('Price Distribution') + theme_bw() 
price_distribution
```
As can be seen, the distribution is right skewed meaning that the very few listings priced at very high price influence the distribution shape whereas we can see from the plot that most of the listings is centered around $100 and is under $200.  From the summary info before, we know that the median is $114 and mean is $189 which is pretty affordable keeping in mind that this is the capital city of the United States.  Effects from COVID-19 probably influenced the cost to be lower than what's considered normal in a metropolitan area.  The limitation of the dataset is that only the past 12 months were provided by the organization so I was limited to doing analysis for the current year of April 2020 - April 2021.  


Which Room type has the most listings? 
```{r}
room_type_count =listings %>% group_by(room_type) %>% summarise(count = sum(n())) %>% arrange(desc(count)) %>% ggplot(aes(x=reorder(room_type, -count), y=count)) + geom_bar(stat='identity', color = 'lightcoral', fill = 'lightcoral') + ylab('# of Listings') + xlab('Room Type') + ggtitle('72% of Listings are Entire Home/Apt') + theme_bw() 
room_type_count
```
Let's look at the Property/Room Type and how it affects price distribution:  

```{r}
price_density_room_type = listings %>% filter(price < 300) %>% ggplot(aes(x=price, group = room_type, fill=room_type)) + geom_density(adjust = 1.5, alpha =.4) + xlab('Price ($)') + ylab('% of Total') + ggtitle('Price Density by Room Type') + theme_bw() 
price_density_room_type
```

What is the average price per Neighborhood?  

```{r}
high_10_price = listings %>% group_by(neighbourhood) %>% summarise(avg_price = mean(price)) %>% arrange(desc(avg_price)) %>% top_n(10)

high_10_price %>%
  ggplot(aes(x=reorder(neighbourhood, avg_price), y=avg_price)) + geom_bar(stat='identity', color = 'palegreen3', fill = 'palegreen3') + coord_flip() + geom_text(aes(label= round(avg_price)), nudge_y=-50, color='black') + ylab('Avg price ($)') + xlab('Neighborhood') + ggtitle('Neighborhoods w. the Highest Avg. Price') + theme_bw() + theme(text=element_text(size = 8))

```


```{r}
low_10_price = listings %>% group_by(neighbourhood) %>% summarise(avg_price = mean(price)) %>% slice_min(avg_price, n =10)

low_10_price %>%
  ggplot(aes(x=reorder(neighbourhood, -avg_price), y=avg_price)) + geom_bar(stat='identity', color = 'palegreen3', fill = 'palegreen3') + scale_y_continuous(limits = c(0, 200)) + coord_flip() + geom_text(aes(label= round(avg_price)), nudge_y=-20, color='black') + ylab('Avg price ($)') + xlab('Neighborhood') + ggtitle('Neighborhoods w. the Lowest Avg. Price') + theme_bw() + theme(text=element_text(size = 8))
```

Since the dataset was missing Neighborhood Group information making it difficult to group the 39 neighborhoods present in the data to show trends in the landscape, I searched and found that Washington, D.C groups its neighborhoods in 8 wards, historically.  So I first populated the neighborhoods data file containing the 39 neighborhood names with the corresponding ward based off of Wikipedia and publicly available city information.  Then I joined it to the main listings dataset to capture the ward/neighborhood grouping factor as a new field.  

```{r}
neighbourhoods = read.csv('./neighbourhoods.csv')
head(neighbourhoods, 2)
listings = inner_join(listings, neighbourhoods, by = 'neighbourhood')
head(listings, 2)
```
Now that we have ward info - a way to group the neighborhoods, let's see the average price by the ward: 

```{r}
ward_avg_price = listings %>% group_by(ward) %>% summarise(avg_price = mean(price)) %>% arrange(avg_price)
ward_avg_price %>%
  ggplot(aes(x=reorder(ward, avg_price), y=avg_price)) + geom_bar(stat='identity', color = 'palegreen3', fill = 'palegreen3')  + geom_text(aes(label= round(avg_price)), nudge_y=-20, color='black') + ylab('Avg price ($)') + xlab('Neighborhood Group') + ggtitle('Average Price by Ward (Neighborhood Group)') + theme_bw()
```
From reading about D.C and knowing a little about certain parts of the city, this makes sense that the closer you get to the Downtown area(Ward 2) with the historic monuments and landmarks(Ward 2) along with affluent residential area(Ward 3) and government buildings(Ward 6) where most of the large events and public gatherings take place the pricier the listings get. On the other hand, Wards 5,7,8 are located furthest away from the Downtown area and can be shown to have the lowest average price per day from the graph above. 


Let's see how much of the Market Size each Ward makes up - by multiplying the available days per year of each listing with its price and summing up by the ward: 

```{r}
listings = listings %>% mutate(availability_price = availability * price)

market_size = sum(listings$availability_price)

market_size_by_ward = listings %>% group_by(ward) %>% summarise(ward_market_size = sum(availability_price), ward_ratio = round(ward_market_size/market_size*100)) %>% arrange(desc(ward_market_size))

market_size_by_ward
```
Let's visualize it: 

```{r}
market_size_by_ward %>% ggplot(aes(fill = ward, y=ward_ratio, x='')) + 
    geom_bar(position="fill", stat="identity") + scale_y_continuous(labels=scales::percent) + ylab('Ward %') + xlab('Market Size') + ggtitle('Market Size by Ward % - Top 3 wards make up 75% !') + theme_bw() 
```
It's interesting to see from the Market Size % stacked Barchart that the top 3 wards - Wards 2, 6, and 1 - contribute 75% of the total Market size.  These same 3 wards are located closest to the Downtown area of Washington D.C supporting the earlier trend seen in Average Price by Neighborhood Group Barplot.  

Let's see if this emerging trend is related to Reviews - there are 2 kinds of review fields - 1st being number of reviews-to-date for the listing and the 2nd being reviews the listing has in the last 12 months, latter of which I missed to capture from the listings_detailed dataset earlier so I am going to capture it now and join it to our main dataset to add to the analysis.  

This is actually quite important as the reviews-to-date field will represent how actively occupied and popular the listing has been ever since it has been active on the market which could be for years.  Whereas for the reviews-in-the-last-12-months field will be more indicative of how likely the listing is currently to be in demand for bookings.  Therefore the reviews field for the last 12 months will be used to compute the likelihood of listings to be in demand when compared to other listings.    

```{r}
last_yr_reviews = listings_detailed %>% select('id', 'number_of_reviews_ltm')
listings = inner_join(listings, last_yr_reviews, by = 'id') 
listings
```
Now that we have both types of review counts, let's first see the Reviews-to-date for listings by each Neighborhood Group (ward): 

```{r}
total_reviews = sum(listings$number_of_reviews)
reviews_by_ward = listings %>% group_by(ward) %>% summarise(ward_reviews = sum(number_of_reviews), ward_review_ratio=round(ward_reviews/total_reviews*100)) %>% arrange(desc(ward_reviews))

reviews_by_ward %>%
  ggplot(aes(x=reorder(ward, -ward_review_ratio), y=ward_review_ratio)) + geom_bar(stat='identity', color = 'cadetblue2', fill = 'cadetblue2')  + geom_text(aes(label= paste0(ward_review_ratio, '%')), nudge_y= -1, color='black') + ylab('Review To Date %') + xlab('Neighborhood Group') + ggtitle('Reviews To Date % by Neighborhood Group') + theme_bw()

```

The same top 3 wards(Wards 6,2,1) that made up 75% of the total Market Size also account for 67% of All Reviews to date.  This makes perfect sense as one would naturally expect most of the reviews to come from 3/4ths of the Market.  

How about Ratings? - since there is minimal and insignificant difference among the Neighborhood Groups, we will not explore further.

```{r}
ratings_by_ward = listings %>% filter(!is.na(rating)) %>% group_by(ward) %>% summarise(ward_rating = mean(rating)) %>% arrange(desc(ward_rating))
ratings_by_ward
```
Let's see the trend for the Reviews of last 12 months by Neighborhood Group:
```{r}
total_reviews_ltm = sum(listings$number_of_reviews_ltm)

reviews_ltm_by_ward = listings %>% group_by(ward) %>% summarise(ward_reviews_ltm = sum(number_of_reviews_ltm), ward_review_ltm_ratio=round(ward_reviews_ltm/total_reviews_ltm*100)) %>% arrange(desc(ward_reviews_ltm))

reviews_ltm_by_ward %>%
  ggplot(aes(x=reorder(ward, -ward_review_ltm_ratio), y=ward_review_ltm_ratio)) + geom_bar(stat='identity', color = 'deepskyblue', fill = 'deepskyblue')  + geom_text(aes(label= paste0(ward_review_ltm_ratio, '%')), nudge_y= -1, color='white') + ylab('Last 12 months Review %') + xlab('Neighborhood Group') + ggtitle('Last 12 months Review % by Neighborhood Group') + theme_bw()
```
The last 12 months Review % shows that the most current reviews trends haven't changed much from the Reviews To Date as the top 4 wards have stayed the same contributing about the same % of the Reviews.  Ward 5 has gone up a place by owning 2% more reviews in the last year compared to Ward 1 so this is good to know from a potential host standpoint.  

Now that we have looked at the trends in Reviews, I can finally answer my 1st question of interest: As a potential guest, Ward 1 and Ward 5 (as a second option) seems to offer the most affordable price on average (potential savings of $83 per day) while being considerably frequently reviewed and located closest to the ward with the downtown area with all the historic landmarks.  Ward 1 and Ward 5 as a backup option also makes up a sizeable portion of the Market size which means potential guests will have enough variety listings to choose from.  So here are the Neighborhoods in Ward 1 and Ward 5 that you can search to get a sweet deal showing their average daily price:

```{r}
unique(listings$ward)
listings = listings %>% mutate(ward = gsub("Ward 1 ", "Ward 1", ward))
``` 

Ward 1 (Best option for guest - closer to downtown): 

```{r}

listings %>% filter(ward == "Ward 1") %>% group_by(neighbourhood) %>% summarise(avg_price = mean(price)) %>% arrange(avg_price) %>%
  ggplot(aes(x=reorder(neighbourhood, -avg_price), y=avg_price)) + geom_bar(stat='identity', color = 'lightcoral', fill = 'lightcoral') +  coord_flip() + geom_text(aes(label= round(avg_price)), nudge_y=-10, color='white') + ylab('Avg price ($)') + xlab('Neighborhood') + ggtitle('Ward 1 Average Price by Neighborhood') + theme_bw() + theme(text=element_text(size = 8))

 
```

Ward 5 (Alternative Better option for guest): 

```{r}
listings %>% filter(ward == "Ward 5") %>% group_by(neighbourhood) %>% summarise(avg_price = mean(price)) %>% arrange(avg_price) %>%
  ggplot(aes(x=reorder(neighbourhood, -avg_price), y=avg_price)) + geom_bar(stat='identity', color = 'lightcoral', fill = 'lightcoral') +  coord_flip() + geom_text(aes(label= round(avg_price)), nudge_y=-10, color='white') + ylab('Avg price ($)') + xlab('Neighborhood') + ggtitle('Ward 5 Average Price by Neighborhood)') + theme_bw() + theme(text=element_text(size = 8))

```

Let's determine where the likelihood of a listing is to be more in demand by taking the ratio between the last 12 month current Reviews and its availability by neighborhood group (ward).  Using the last 12 month review count as an indicator of likelihood of occupancy and the listing available days as a factor of meeting the market demand: 
```{r}
total_reviews_ltm = sum(listings$number_of_reviews_ltm)
total_availability = sum(listings$availability)
likelihood = listings %>% group_by(ward) %>% summarise(ward_reviews_ltm = sum(number_of_reviews_ltm), ward_availability = sum(availability), reviews_per_availability = ward_reviews_ltm/ward_availability) %>% arrange(desc(reviews_per_availability))
likelihood
```
Here I can address the 2nd Question of my interest: Ward 5 and 1 would be the wards I would look into potentially put a listing because they have the highest likelihood for demand currently and they are good pick for people who are looking for affordable options compared to Downtown area listings which are more expensive.  Ward 7 isn't recommended because it only makes up 3% of market size having less information to make an investment decision. However on the other hand I guess it could be an option due to less competition so we may keep an eye on it.  

Visualization:
```{r}
listings %>% group_by(ward) %>% summarise(ward_reviews_ltm = sum(number_of_reviews_ltm), ward_availability = sum(availability), reviews_per_availability = ward_reviews_ltm/ward_availability) %>% arrange(desc(reviews_per_availability)) %>%
  
  ggplot(aes(x=reorder(ward, -reviews_per_availability ), y=reviews_per_availability)) + geom_bar(stat='identity', color = 'deepskyblue', fill = 'deepskyblue') + ylab('Reviews per availability') + xlab('Neighborhood Group') + ggtitle('Likelihood of Demand by Neighborhood Group') + theme_bw() 

  
  # ggplot(aes(x=reorder(ward, -reviews_per_availability), y=reviews_per_availability)) + geom_bar(stat='identity', color = 'deepskyblue', fill = 'deepskyblue') + ylab('Reviews per availability') + xlab('Neighborhood Group') + ggtitle('Likelihood of Demand by Neighborhood Group) + theme_bw()
```
Next Steps:

- Obtain data from previous years for better comparison and trends over the years
- Build a predictive model to apply to new data 
- Complete the Shiny app with interactive Map features

```{r}
listings_shiny = listings 
write.csv(listings_shiny,"C:/Users/oyb50/Documents/Augi/NYC DS Academy/Bootcamp/Projects/Shiny Project/Airbnb-Shiny/listings_shiny.csv", row.names = FALSE)

```


 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
