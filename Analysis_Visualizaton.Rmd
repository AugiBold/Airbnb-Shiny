---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
```
Airbnb, more than just a vacation rental company, was one of the market disruptors that started in 2008 which destabilized the hotel industry and changed the way people traveled by offering more options in booking places to stay.  It got so popular that more and more people started using it as a business which eventually led to certain cities to put in place regulations on short term rentals.  

One of those cities is Washington, D.C and I obtained my data from insideairbnb.com, an independent non-commercial project that gathers publicly available Airbnb information with the express purpose of promoting awareness on this issue of Airbnb adversely affecting local neighborhoods and established real-estate businesses.  The dataset only provided listings that were available for the last 12 months in Washington, D.C.  With that in mind, through this project I wanted to explore 2 questions particularly in the current post Covid state:

1) As a potential guest, where are the more affordable yet well rated and reviewed listings? 
2) As a potential host, where is the most demand in the D.C market for listings? 

The main data I will be working with is the listings data and it includes all kinds of data from which I selected data regarding the following:listings, coordinate location, neighborhood, price per night, room type, number of reviews, review scores, and availability days per year.

```{r}
listings = read.csv('./listings.csv')
listings_detailed = read.csv('./listings_detailed.csv')
head(listings)
head(listings_detailed)

```
I first select data fields pertaining to my question of interest from the main dataset and also extract the review-score data from the listings_detailed dataset and join it to the main dataset to capture the rating. 
```{r}
listings = listings %>% select('id', 'name', 'host_id', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'number_of_reviews', 'calculated_host_listings_count', 'availability_365') 
head(listings, 3)
```   
Join the data and clean up the long names into short, to-the-point names for the columns.  
```{r}
rating = listings_detailed %>% select('id', 'review_scores_rating')
listings = inner_join(listings, rating, by = 'id')
listings = listings %>% rename(host_listings = 'calculated_host_listings_count', availability = 'availability_365', rating = 'review_scores_rating')
head(listings)
```

Let's get a general sense of the data:
```{r}
str(listings)
summary(listings)
colSums(is.na(listings))
```
As we can see, the rating field has 2373 NULL/NaN values making up about 30% of all the data.  
For general analysis, we will keep the data for sure, but when we are considering rating alone, we will omit the null value.  
This sort of correlates with the number of reviews having 0 values when checked: 

```{r}
listings %>% filter(number_of_reviews==0) %>% summarise(count = n())
```
Noticed that there were 6 listings with $0 price which doesn't help our analysis so I excluded them: 
```{r}
listings = listings %>% filter(price > 0)
dim(listings)
```

Let's look at the General Distribution of the Prices to get a feel for the pricing landscape:
Because 94% of listings fall under $500 per night, I plotted the distribution within this range for better visibility. 

```{r}
listings %>% filter(price < 500) %>% summarise(count = n(), under_500_pct = count/length(listings$price)*100) 
```


```{r}
listings %>% filter(price < 500) %>% ggplot(aes(x = price)) +
  geom_histogram(aes(y = ..density..), color = "blue", fill = "darkseagreen3") + geom_density(alpha = .2, fill="chartreuse") + xlab('Price ($)') + ylab('Density') + ggtitle('Price Distribution') + theme_bw() 
```
As can be seen, the distribution is right skewed meaning that the very few listings priced at very high amounts influence the shape whereas we can see from the plot that most of the listings is centered around $100 and is under $200.  From the summary info before, we know that the median is $114 and mean is $189 which is pretty affordable keeping in mind that this is the capital city of the United States.  Effects from COVID-19 probably influenced the cost to be lower than what's considered normal in a metropolitan area.  The limitation of the dataset is that only the past 12 months were provided by the organization.  


Which Room type has the most listings? 
```{r}
listings %>% group_by(room_type) %>% summarise(count = sum(n())) %>% arrange(desc(count)) %>% ggplot(aes(x=reorder(room_type, -count), y=count)) + geom_bar(stat='identity', color = 'lightcoral', fill = 'lightcoral') + ylab('# of Listings') + xlab('Room Type') + ggtitle('72% of Listings are Entire Home/Apt') + theme_bw() 
```


Let's look at the Property/Room Type and how it affects price distribution:  

```{r}
listings %>% filter(price < 300) %>% ggplot(aes(x=price, group = room_type, fill=room_type)) + geom_density(adjust = 1.5, alpha =.4) + xlab('Price ($)') + ylab('% of Total') + ggtitle('Price Density by Room Type') + theme_bw() 
```

What is the average price per Neighborhood?  

```{r}
high_10_price = listings %>% group_by(neighbourhood) %>% summarise(avg_price = mean(price)) %>% arrange(desc(avg_price)) %>% top_n(10)

high_10_price %>%
  ggplot(aes(x=reorder(neighbourhood, avg_price), y=avg_price)) + geom_bar(stat='identity', color = 'palegreen3', fill = 'palegreen3') + coord_flip() + geom_text(aes(label= round(avg_price)), nudge_y=-50, color='black') + ylab('Avg price ($)') + xlab('Neighborhood') + ggtitle('Highest Avg. Price by Neighborhood') + theme_bw() + theme(text=element_text(size = 8))

```


```{r}
low_10_price = listings %>% group_by(neighbourhood) %>% summarise(avg_price = mean(price)) %>% slice_min(avg_price, n =10)

low_10_price %>%
  ggplot(aes(x=reorder(neighbourhood, -avg_price), y=avg_price)) + geom_bar(stat='identity', color = 'palegreen3', fill = 'palegreen3') + scale_y_continuous(limits = c(0, 200)) + coord_flip() + geom_text(aes(label= round(avg_price)), nudge_y=-20, color='black') + ylab('Avg price ($)') + xlab('Neighborhood') + ggtitle('Lowest Avg. Price by Neighborhood') + theme_bw() + theme(text=element_text(size = 8))
```

Since the dataset was missing Neighborhood Group information making it difficult to group the 39 neighborhoods present in the data to show trends in the landscape, I searched and found that Washington, D.C groups its neighborhoods in 8 wards, historically.  So I first populated the neighborhoods data file containing the 39 neighborhood names with the corresponding ward based off of Wikipedia and publicly available city information.  Then I joined it to the main listings dataset to capture the ward/neighborhood grouping factor as a new field.  

```{r}
neighbourhoods = read.csv('./neighbourhoods.csv')
head(neighbourhoods, 2)
listings = inner_join(listings, neighbourhoods, by = 'neighbourhood')
head(listings, 2)
```
Now that we have ward info - a way to group the neighborhoods, let's see the average price by the ward: 

```{r}
ward_avg_price = listings %>% group_by(ward) %>% summarise(avg_price = mean(price)) %>% arrange(avg_price)
ward_avg_price %>%
  ggplot(aes(x=reorder(ward, avg_price), y=avg_price)) + geom_bar(stat='identity', color = 'palegreen3', fill = 'palegreen3')  + geom_text(aes(label= round(avg_price)), nudge_y=-20, color='black') + ylab('Avg price ($)') + xlab('Neighborhood Group') + ggtitle('Daily Average Price by Neighborhood') + theme_bw()
```
Being the Capital city, a unique characteristic to Washington D.C is that it is one of the most visited destinations in the world and most who visit it, if not all, visit the landmarks around the National Mall which has been rated as the most trafficked landmark area.  Many public and large events take place for various causes.  Therefore, I wanted to explore if listings located closer to this landmark was more popular or more likely to be booked (demand) and also if there was correlation between the price and the proximity. So I found out the National Mall coordinates (38.8875° N, 77.0364° W) and created a new proximity column to each listings called mall_proximity.  

```{r}
head(listings)
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
